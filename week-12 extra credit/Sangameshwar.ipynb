{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N4tEz4gIYhov"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "#Load data\n",
        "fastfood = pd.read_csv(\"fastfood.csv\")\n",
        "\n",
        "#Drop non-numeric or identifier-like columns\n",
        "X = fastfood.drop(columns=['restaurant', 'item', 'calories', 'salad'], errors='ignore')\n",
        "y = fastfood['calories']\n",
        "\n",
        "#Ensure all features are numeric and drop rows with missing values\n",
        "X = X.apply(pd.to_numeric, errors='coerce')\n",
        "data = pd.concat([X, y], axis=1).dropna()\n",
        "\n",
        "X_clean = data.drop(columns=['calories'])\n",
        "y_clean = data['calories']\n",
        "\n",
        "#Split into training and testing sets (using CLEAN data)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42)\n",
        "\n",
        "#Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "#Build the neural network\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(X_train_scaled.shape[1],)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(1)  # Regression output\n",
        "])\n",
        "\n",
        "#Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "#Train the model\n",
        "model.fit(X_train_scaled, y_train, epochs=50, validation_split=0.2, verbose=1)\n",
        "\n",
        "#Evaluate on test set\n",
        "loss, mae = model.evaluate(X_test_scaled, y_test)\n",
        "print(f\"Mean Absolute Error on Test Set: {mae:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fgebyhdZNoy",
        "outputId": "82605099-3faa-43e8-9822-29f91252ca7e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - loss: 442919.5938 - mae: 584.4946 - val_loss: 292387.0938 - val_mae: 480.6519\n",
            "Epoch 2/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 370250.1875 - mae: 538.9827 - val_loss: 292251.1250 - val_mae: 480.5335\n",
            "Epoch 3/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 372243.1250 - mae: 531.6611 - val_loss: 292118.9688 - val_mae: 480.4198\n",
            "Epoch 4/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 447170.9062 - mae: 566.2889 - val_loss: 291988.2188 - val_mae: 480.3078\n",
            "Epoch 5/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 458059.4688 - mae: 572.0874 - val_loss: 291855.9688 - val_mae: 480.1953\n",
            "Epoch 6/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 408262.5312 - mae: 540.1736 - val_loss: 291722.3438 - val_mae: 480.0836\n",
            "Epoch 7/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 374977.3750 - mae: 532.2979 - val_loss: 291578.5938 - val_mae: 479.9662\n",
            "Epoch 8/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 432438.2188 - mae: 556.1218 - val_loss: 291419.0000 - val_mae: 479.8407\n",
            "Epoch 9/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 451421.0312 - mae: 566.3627 - val_loss: 291241.0625 - val_mae: 479.7039\n",
            "Epoch 10/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 368639.6875 - mae: 533.3011 - val_loss: 291048.5000 - val_mae: 479.5553\n",
            "Epoch 11/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 445793.6250 - mae: 576.8043 - val_loss: 290822.8438 - val_mae: 479.3859\n",
            "Epoch 12/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 387025.8750 - mae: 549.7839 - val_loss: 290581.6562 - val_mae: 479.2059\n",
            "Epoch 13/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 411563.0312 - mae: 557.4178 - val_loss: 290302.0938 - val_mae: 479.0012\n",
            "Epoch 14/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 444553.5938 - mae: 564.1895 - val_loss: 289984.3750 - val_mae: 478.7706\n",
            "Epoch 15/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 362043.2188 - mae: 535.8235 - val_loss: 289641.5312 - val_mae: 478.5216\n",
            "Epoch 16/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 395640.9375 - mae: 538.1790 - val_loss: 289235.9688 - val_mae: 478.2326\n",
            "Epoch 17/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 369087.9062 - mae: 531.9115 - val_loss: 288797.6562 - val_mae: 477.9194\n",
            "Epoch 18/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 425884.6875 - mae: 553.4045 - val_loss: 288282.8438 - val_mae: 477.5523\n",
            "Epoch 19/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 419683.4375 - mae: 563.9321 - val_loss: 287718.7812 - val_mae: 477.1465\n",
            "Epoch 20/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - loss: 387734.3125 - mae: 552.5189 - val_loss: 287100.7188 - val_mae: 476.6972\n",
            "Epoch 21/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 409753.5625 - mae: 555.5735 - val_loss: 286373.3750 - val_mae: 476.1747\n",
            "Epoch 22/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 404383.4062 - mae: 548.5330 - val_loss: 285544.8438 - val_mae: 475.5799\n",
            "Epoch 23/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 436008.4062 - mae: 561.3448 - val_loss: 284582.0938 - val_mae: 474.8946\n",
            "Epoch 24/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 384606.6250 - mae: 537.7859 - val_loss: 283497.0938 - val_mae: 474.1252\n",
            "Epoch 25/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 417372.1875 - mae: 557.2834 - val_loss: 282264.3438 - val_mae: 473.2552\n",
            "Epoch 26/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 405448.7188 - mae: 546.3298 - val_loss: 280915.7812 - val_mae: 472.2996\n",
            "Epoch 27/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 366768.3125 - mae: 530.6050 - val_loss: 279467.9375 - val_mae: 471.2706\n",
            "Epoch 28/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 392110.2812 - mae: 545.1730 - val_loss: 277843.7188 - val_mae: 470.1156\n",
            "Epoch 29/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 425986.3438 - mae: 557.4066 - val_loss: 276068.3750 - val_mae: 468.8508\n",
            "Epoch 30/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 344375.0938 - mae: 517.4832 - val_loss: 274231.5938 - val_mae: 467.5308\n",
            "Epoch 31/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 377965.5312 - mae: 543.9236 - val_loss: 272172.1562 - val_mae: 466.0518\n",
            "Epoch 32/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 375926.7812 - mae: 526.1500 - val_loss: 269944.3438 - val_mae: 464.4449\n",
            "Epoch 33/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 382695.5938 - mae: 538.8625 - val_loss: 267571.7188 - val_mae: 462.7289\n",
            "Epoch 34/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 354512.2812 - mae: 521.5784 - val_loss: 265053.3750 - val_mae: 460.8926\n",
            "Epoch 35/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 403954.2188 - mae: 557.8739 - val_loss: 262363.7812 - val_mae: 458.9226\n",
            "Epoch 36/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 356981.6250 - mae: 510.4374 - val_loss: 259505.8750 - val_mae: 456.8075\n",
            "Epoch 37/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 356871.2188 - mae: 531.1926 - val_loss: 256503.7344 - val_mae: 454.5750\n",
            "Epoch 38/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 316034.1562 - mae: 499.3979 - val_loss: 253366.0469 - val_mae: 452.2176\n",
            "Epoch 39/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 310901.5000 - mae: 497.4134 - val_loss: 249999.2500 - val_mae: 449.6711\n",
            "Epoch 40/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 319505.1250 - mae: 493.5798 - val_loss: 246369.2500 - val_mae: 446.9132\n",
            "Epoch 41/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 349891.7500 - mae: 514.0932 - val_loss: 242521.7656 - val_mae: 443.9660\n",
            "Epoch 42/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 319454.6875 - mae: 505.0955 - val_loss: 238664.3906 - val_mae: 440.9646\n",
            "Epoch 43/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 314437.4062 - mae: 496.4054 - val_loss: 234561.2656 - val_mae: 437.7440\n",
            "Epoch 44/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 297441.1562 - mae: 493.4997 - val_loss: 230345.4375 - val_mae: 434.3849\n",
            "Epoch 45/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 296542.0625 - mae: 489.4268 - val_loss: 225915.7500 - val_mae: 430.8238\n",
            "Epoch 46/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 304592.3125 - mae: 485.0988 - val_loss: 221192.6250 - val_mae: 426.9725\n",
            "Epoch 47/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 295937.3750 - mae: 494.5447 - val_loss: 216437.1250 - val_mae: 423.0197\n",
            "Epoch 48/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 266512.0000 - mae: 457.9337 - val_loss: 211483.0469 - val_mae: 418.8349\n",
            "Epoch 49/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 261029.8906 - mae: 464.5478 - val_loss: 206387.5469 - val_mae: 414.4502\n",
            "Epoch 50/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 259079.6406 - mae: 458.2100 - val_loss: 201081.5156 - val_mae: 409.8063\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 223549.4062 - mae: 442.0663\n",
            "Mean Absolute Error on Test Set: 443.01\n"
          ]
        }
      ]
    }
  ]
}