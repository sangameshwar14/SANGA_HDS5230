{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIDypzPG5AHu",
        "outputId": "ee182679-0fa4-45c0-cae2-711aa262bdb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data size | Configuration | Training error | Validation error | Time of execution\n",
            "--------------------------------------------------------------------------------\n",
            "    1000 | 1 hidden layer 4 nodes         | 0.004000 | 0.015130 | 1.097652\n",
            "   10000 | 1 hidden layer 4 nodes         | 0.001100 | 0.001515 | 3.505305\n",
            "  100000 | 1 hidden layer 4 nodes         | 0.000920 | 0.001012 | 14.969793\n",
            "    1000 | 2 hidden layers of 4 nodes each | 0.002000 | 0.006722 | 1.207694\n",
            "   10000 | 2 hidden layers of 4 nodes each | 0.000300 | 0.001406 | 3.405355\n",
            "  100000 | 2 hidden layers of 4 nodes each | 0.000580 | 0.000551 | 22.005155\n",
            "\n",
            "Results Summary:\n",
            "   Data size                    Configuration  Training error  \\\n",
            "0       1000           1 hidden layer 4 nodes         0.00400   \n",
            "1      10000           1 hidden layer 4 nodes         0.00110   \n",
            "2     100000           1 hidden layer 4 nodes         0.00092   \n",
            "3       1000  2 hidden layers of 4 nodes each         0.00200   \n",
            "4      10000  2 hidden layers of 4 nodes each         0.00030   \n",
            "5     100000  2 hidden layers of 4 nodes each         0.00058   \n",
            "\n",
            "   Validation error  Time of execution  \n",
            "0          0.015130           1.097652  \n",
            "1          0.001515           3.505305  \n",
            "2          0.001012          14.969793  \n",
            "3          0.006722           1.207694  \n",
            "4          0.001406           3.405355  \n",
            "5          0.000551          22.005155  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the data\n",
        "pima_df = pd.read_csv(\"week_11_pima.csv\")\n",
        "\n",
        "# Prepare the data\n",
        "X = pima_df.drop('outcome', axis=1)\n",
        "y = pima_df['outcome']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Function to train and evaluate models\n",
        "def train_and_evaluate(X_train, y_train, X_val, y_val, hidden_layers, max_samples=None):\n",
        "    if max_samples:\n",
        "        # Limit the training data size\n",
        "        X_train_subset = X_train[:max_samples]\n",
        "        y_train_subset = y_train[:max_samples]\n",
        "    else:\n",
        "        X_train_subset = X_train\n",
        "        y_train_subset = y_train\n",
        "\n",
        "    # Create and train the model\n",
        "    start_time = time.time()\n",
        "\n",
        "    model = MLPClassifier(\n",
        "        hidden_layer_sizes=hidden_layers,\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        max_iter=500,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_subset, y_train_subset)\n",
        "\n",
        "    # Calculate training and validation accuracy\n",
        "    train_pred = model.predict(X_train_subset)\n",
        "    val_pred = model.predict(X_val)\n",
        "\n",
        "    train_error = 1 - accuracy_score(y_train_subset, train_pred)\n",
        "    val_error = 1 - accuracy_score(y_val, val_pred)\n",
        "\n",
        "    execution_time = time.time() - start_time\n",
        "\n",
        "    return train_error, val_error, execution_time\n",
        "\n",
        "# Run experiments\n",
        "results = []\n",
        "\n",
        "# Define configurations\n",
        "configs = [\n",
        "    (1000, (4,), \"1 hidden layer 4 nodes\"),\n",
        "    (10000, (4,), \"1 hidden layer 4 nodes\"),\n",
        "    (100000, (4,), \"1 hidden layer 4 nodes\"),\n",
        "    (1000, (4, 4), \"2 hidden layers of 4 nodes each\"),\n",
        "    (10000, (4, 4), \"2 hidden layers of 4 nodes each\"),\n",
        "    (100000, (4, 4), \"2 hidden layers of 4 nodes each\")\n",
        "]\n",
        "\n",
        "print(\"Data size | Configuration | Training error | Validation error | Time of execution\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for data_size, hidden_layers, config_desc in configs:\n",
        "    # For datasets larger than available, create synthetic data by resampling\n",
        "    if data_size > len(X_train):\n",
        "        # Create synthetic data by resampling with replacement\n",
        "        indices = np.random.choice(len(X_train), size=data_size, replace=True)\n",
        "        X_train_resampled = X_train_scaled[indices]\n",
        "        y_train_resampled = y_train.iloc[indices]\n",
        "\n",
        "        train_error, val_error, exec_time = train_and_evaluate(\n",
        "            X_train_resampled, y_train_resampled, X_val_scaled, y_val,\n",
        "            hidden_layers\n",
        "        )\n",
        "    else:\n",
        "        train_error, val_error, exec_time = train_and_evaluate(\n",
        "            X_train_scaled, y_train, X_val_scaled, y_val,\n",
        "            hidden_layers, max_samples=data_size\n",
        "        )\n",
        "\n",
        "    print(f\"{data_size:8d} | {config_desc:30s} | {train_error:.6f} | {val_error:.6f} | {exec_time:.6f}\")\n",
        "\n",
        "    results.append({\n",
        "        'Data size': data_size,\n",
        "        'Configuration': config_desc,\n",
        "        'Training error': train_error,\n",
        "        'Validation error': val_error,\n",
        "        'Time of execution': exec_time\n",
        "    })\n",
        "\n",
        "# Convert results to DataFrame for better visualization\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nResults Summary:\")\n",
        "print(results_df)"
      ]
    }
  ]
}